from typing import List, Tuple, Optional, Union

import torch
from sentence_transformers import CrossEncoder


class CrossEncoderReranker:

    def __init__(
            self,
            model_name_or_path: str = "../data/models_data/cross-encoder/ms-marco-MiniLM-L6-v2",
            device: Optional[str] = None,
            batch_size: int = 16
    ):
        """
        åˆå§‹åŒ–é‡æ’å™¨ã€‚
        åŸºäºCrossEncoderçš„æ–‡æ¡£é‡æ’å™¨ï¼Œç”¨äºæ£€ç´¢ç³»ç»Ÿä¸­å¯¹å¬å›é˜¶æ®µçš„å€™é€‰æ–‡æ¡£è¿›è¡Œè¯­ä¹‰ç›¸å…³æ€§é‡æ’ï¼Œæå‡æ£€ç´¢ç²¾åº¦ã€‚
        é€‚é…éµå¾ª sentence-transformers çš„ CrossEncoder è§„èŒƒçš„æ¨¡å‹, å¦‚ms-marco-MiniLM-L-12-v2ã€bge-reranker-v2-m3ç­‰

        Args:
            model_name_or_path: strï¼Œæ¨¡å‹åç§°ï¼ˆHuggingFace Hubè§„èŒƒåï¼‰æˆ–æœ¬åœ°å­˜å‚¨è·¯å¾„ï¼š
                - 1.æ¨¡å‹åç§°ï¼šæœ¬åœ°ç¼“å­˜ï¼ˆé»˜è®¤~/.cache/huggingface/ï¼‰æ— è¯¥æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨ä»Hubä¸‹è½½æƒé‡/é…ç½®/åˆ†è¯å™¨ï¼›ç¼“å­˜å·²å­˜åœ¨åˆ™ç›´æ¥åŠ è½½ï¼Œæ— éœ€é‡å¤ä¸‹è½½ã€‚
                - 2.æœ¬åœ°è·¯å¾„ï¼šéœ€æ‰‹åŠ¨ä¸‹è½½å®Œæ•´æ¨¡å‹æ–‡ä»¶ï¼ˆåŒ…å«config.jsonã€model.safetensors/pytorch_model.binç­‰ï¼‰åˆ°æœ¬åœ°è·¯å¾„åœ°å€ã€‚
            device: æ¨¡å‹è¿è¡Œè®¾å¤‡ï¼ŒNoneåˆ™è‡ªåŠ¨æ£€æµ‹ï¼ˆä¼˜å…ˆä½¿ç”¨CUDAï¼Œæ— åˆ™ä½¿ç”¨CPUï¼‰
            batch_size: æ¨ç†æ‰¹æ¬¡å¤§å°ï¼Œå»ºè®®CPUè®¾8/16ï¼ŒGPUå¯æ ¹æ®æ˜¾å­˜é€‚å½“å¢å¤§ï¼ˆé»˜è®¤16ï¼‰
        """
        # è®¾å¤‡è‡ªåŠ¨é€‚é…
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.model_name_or_path = model_name_or_path
        self.model: Optional[CrossEncoder] = None

        # åˆå§‹åŒ–æ¨¡å‹
        self._load_model()

    def _load_model(self) -> None:
        """åŠ è½½å¹¶é‡åˆå§‹åŒ–CrossEncoderé‡æ’æ¨¡å‹"""
        try:
            self.model = CrossEncoder(self.model_name_or_path, device=self.device)
            print(f"âœ…é‡æ’æ¨¡å‹åŠ è½½å®Œæˆ | è®¾å¤‡ï¼š{self.device} | æ‰¹æ¬¡å¤§å°ï¼š{self.batch_size}")
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}\nè¯·æ£€æŸ¥ï¼š1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡® 2. ç½‘ç»œæ˜¯å¦æ­£å¸¸ï¼ˆé¦–æ¬¡ä¸‹è½½éœ€è”ç½‘ï¼‰")

    def rerank_sorted(
            self,
            query: str,
            candidate_docs: List[str],
            return_scores: bool = True,
            top_k: Optional[int] = None
    ) -> Union[List[str], List[Tuple[str, float]]]:
        """
        å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œç›¸å…³æ€§é‡æ’

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢è¯­å¥
            candidate_docs: å¬å›é˜¶æ®µçš„å€™é€‰æ–‡æ¡£åˆ—è¡¨ï¼ˆå»ºè®®top-20ä»¥å†…ï¼Œä¿è¯é‡æ’æ•ˆç‡ï¼‰
            return_scores: æ˜¯å¦è¿”å›ç›¸å…³æ€§å¾—åˆ†ï¼ˆå¾—åˆ†è¶Šé«˜ï¼Œç›¸å…³æ€§è¶Šå¼ºï¼‰
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸å…³æ–‡æ¡£ï¼ŒNoneè¿”å›å…¨éƒ¨

        Returns:
            é‡æ’åçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆæˆ–æ–‡æ¡£+å¾—åˆ†çš„å…ƒç»„åˆ—è¡¨ï¼‰ï¼ŒæŒ‰ç›¸å…³æ€§é™åºæ’åˆ—

        Raises:
            ValueError: å€™é€‰æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºæ—¶æŠ›å‡º
            RuntimeError: æ¨¡å‹æœªæˆåŠŸåŠ è½½æ—¶æŠ›å‡º
        """
        # è¾“å…¥åˆæ³•æ€§æ ¡éªŒ
        if not candidate_docs:
            raise ValueError("å€™é€‰æ–‡æ¡£åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åŠ è½½æ˜¯å¦æˆåŠŸ")

        # æ„é€ query-docé…å¯¹ï¼ˆCrossEncoderå¿…éœ€è¾“å…¥æ ¼å¼ï¼‰
        doc_pairs = [[query, doc] for doc in candidate_docs]

        # æ‰¹é‡æ¨ç†è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        try:
            scores = self.model.predict(doc_pairs, batch_size=self.batch_size)
        except Exception as e:
            raise RuntimeError(f"é‡æ’æ‰“åˆ†å¤±è´¥ï¼š{e}")

        # æŒ‰å¾—åˆ†é™åºæ’åº
        sorted_indices = scores.argsort()[::-1]
        ranked_results = [
            (candidate_docs[i], float(scores[i])) for i in sorted_indices
        ]

        # æˆªå–top_kç»“æœ
        if top_k is not None and top_k > 0:
            ranked_results = ranked_results[:top_k]

        # é€‚é…è¿”å›æ ¼å¼
        if return_scores:
            return ranked_results
        else:
            return [item[0] for item in ranked_results]

    def release(self):
        """æ˜¾å¼é‡Šæ”¾æ¨¡å‹èµ„æºï¼ˆå»ºè®®æ‰‹åŠ¨è°ƒç”¨ï¼‰"""
        if self.model is not None:
            self.model = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("ğŸ”Œ é‡æ’æ¨¡å‹èµ„æºå·²é‡Šæ”¾")


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # 1. åˆ›å»ºé‡æ’å™¨å®ä¾‹
    reranker = CrossEncoderReranker(
        # model_name_or_path="cross-encoder/ms-marco-MiniLM-L6-v2",  # ä½¿ç”¨HuggingFaceè¿œç¨‹åŠ è½½æ¨¡å‹
        # æ‰‹åŠ¨ä»huggingface.coæˆ–modelscope.cnä¸­ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œå­˜å‚¨æœ¬åœ°è·¯å¾„ä¸‹
        model_name_or_path="../../data/models_reranker_data/cross-encoder/ms-marco-MiniLM-L6-v2",
        # device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
        batch_size=16
    )

    # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
    query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
    docs = [
        "äººå·¥æ™ºèƒ½é©±åŠ¨çš„å†œä¸šæ— äººæœºå¯å®ç°ä½œç‰©ç—…è™«å®³ç›‘æµ‹ï¼Œç²¾å‡†ç‡è¾¾85%ä»¥ä¸Š-E",  # æ— å…³ï¼ˆå®Œå…¨è·¨é¢†åŸŸï¼‰
        "AIè¾…åŠ©è¯Šæ–­ç³»ç»Ÿå¯å¿«é€Ÿåˆ†æCTã€MRIç­‰åŒ»å­¦å½±åƒï¼Œä½¿è‚ºç™Œã€ä¹³è…ºç™Œç­‰ç–¾ç—…çš„æ—©æœŸæ£€å‡ºç‡æå‡30%ä»¥ä¸Š-A",  # é«˜ç›¸å…³
        "æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨é‡‘èé£æ§ä¸­çš„åº”ç”¨æ¡ˆä¾‹ï¼Œæœ‰æ•ˆé™ä½ä¿¡è´·è¿çº¦ç‡15%-E",  # æ— å…³ï¼ˆé‡‘èé¢†åŸŸï¼‰
        "å·ç§¯ç¥ç»ç½‘ç»œåœ¨çš®è‚¤ç™Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°91%çš„å‡†ç¡®ç‡ï¼Œä¸ºåŸºå±‚åŒ»ç–—æœºæ„æä¾›äº†ä½æˆæœ¬è¯Šæ–­æ–¹æ¡ˆ-B",  # ä¸­é«˜ç›¸å…³ï¼ˆæŠ€æœ¯ç»†åˆ†ï¼Œé—´æ¥ç›¸å…³ï¼‰
        "æ™ºèƒ½æ¨¡æ‹Ÿåœ¨ç”Ÿç‰©åŒ»è¯ç ”å‘ä¸­çš„åº”ç”¨ï¼Œä¸»è¦ç”¨äºè¯ç‰©åˆ†å­ç»“æ„æ¨¡æ‹Ÿï¼Œä¸å±äºä¸´åºŠåŒ»ç–—åº”ç”¨èŒƒç•´-D",  # å¹²æ‰°é¡¹ï¼ˆè¯­ä¹‰ç›¸è¿‘ä½†é¢†åŸŸä¸ç¬¦ï¼‰
        "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„æ ¸å¿ƒåº”ç”¨åŒ…æ‹¬AIè¾…åŠ©è¯Šæ–­ã€ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆç”Ÿæˆã€åŒ»ç–—å½±åƒåˆ†æç­‰-A",  # é«˜ç›¸å…³
        "æœºå™¨å­¦ä¹ ç®—æ³•åœ¨åŒ»ä¿é£æ§ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œå¯è¯†åˆ«è™šå‡å°±åŒ»æŠ¥é”€è¡Œä¸ºï¼Œå±äºAIåœ¨åŒ»ç–—ç®¡ç†çš„è¾¹ç¼˜åœºæ™¯-D",  # ä½ç›¸å…³ï¼ˆè·¨é¢†åŸŸAIï¼Œå¼±å…³è”ï¼‰
        "åŒ»ç–—å¤§æ•°æ®å¹³å°é€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•æ•´åˆæ‚£è€…ç”µå­ç—…å†ï¼Œä¸ºåŒ»é™¢ç®¡ç†å†³ç­–æä¾›æ•°æ®æ”¯æ’‘-C",  # ä¸­ç›¸å…³ï¼ˆæ³›åŒ»ç–—AIï¼Œå…³è”æ€§å‡å¼±ï¼‰
    ]

    # 3. æ‰§è¡Œé‡æ’
    # è¿”å›å¸¦å¾—åˆ†çš„ç»“æœ
    ranked_docs_with_scores = reranker.rerank_sorted(
        query=query,
        candidate_docs=docs,
        return_scores=True,
        top_k=None  # å¯æŒ‡å®šè¿”å›å‰Nä¸ªï¼Œå¦‚top_k=2
    )

    # 4. è¾“å‡ºç»“æœ
    print("\n=== ms-marco-MiniLM-L6-v2 é‡æ’ç»“æœ===")
    print(f"query:{query}")
    for idx, (doc, score) in enumerate(ranked_docs_with_scores, 1):
        print(f"TOP-{idx} | å¾—åˆ†ï¼š{score:.4f}  æ–‡æ¡£ï¼š{doc}")
